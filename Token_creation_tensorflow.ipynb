{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab5c8623",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd607a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61c7004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = [\n",
    "    'I am good human as i am with good dog'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b33f3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d861bf53",
   "metadata": {},
   "source": [
    "# This class allows to vectorize a text corpus, by turning each text into either a sequence of integers \n",
    "\n",
    "## The Number of Words That it can Tokenize.\n",
    "\n",
    "#### the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
    "#f.keras.preprocessing.text.Tokenizer(num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' ', char_level=False, oov_token=None,\n",
    "    document_count=0, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a2056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58177e5d",
   "metadata": {},
   "source": [
    "# fit_on_texts will create a tokenized word index printing a set of KEY/Value pairs for the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8111b5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'am': 2, 'good': 3, 'human': 4, 'as': 5, 'with': 6, 'dog': 7}\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec5e639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_words': 100,\n",
       " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
       " 'lower': True,\n",
       " 'split': ' ',\n",
       " 'char_level': False,\n",
       " 'oov_token': None,\n",
       " 'document_count': 1,\n",
       " 'word_counts': '{\"i\": 2, \"am\": 2, \"good\": 2, \"human\": 1, \"as\": 1, \"with\": 1, \"dog\": 1}',\n",
       " 'word_docs': '{\"good\": 1, \"human\": 1, \"dog\": 1, \"with\": 1, \"i\": 1, \"am\": 1, \"as\": 1}',\n",
       " 'index_docs': '{\"3\": 1, \"4\": 1, \"7\": 1, \"6\": 1, \"1\": 1, \"2\": 1, \"5\": 1}',\n",
       " 'index_word': '{\"1\": \"i\", \"2\": \"am\", \"3\": \"good\", \"4\": \"human\", \"5\": \"as\", \"6\": \"with\", \"7\": \"dog\"}',\n",
       " 'word_index': '{\"i\": 1, \"am\": 2, \"good\": 3, \"human\": 4, \"as\": 5, \"with\": 6, \"dog\": 7}'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc53c082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c9e7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sen1 = [\n",
    "    'Today is a sunny day',\n",
    "    'Today is a autum day',\n",
    "    'what is it like today'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "002f65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71716e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'today': 1, 'is': 2, 'a': 3, 'day': 4, 'sunny': 5, 'autum': 6, 'what': 7, 'it': 8, 'like': 9}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(Sen1)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3f651aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = [\n",
    "    'How are % today.',\n",
    "    'I love $ money:'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf86af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'how': 1, 'are': 2, '%': 3, 'today.': 4, 'i': 5, 'love': 6, '$': 7, 'money:': 8}\n"
     ]
    }
   ],
   "source": [
    "tokenizer_1 = Tokenizer(num_words = 100, filters = '')\n",
    "tokenizer_1.fit_on_texts(sen)\n",
    "word_index_1 = tokenizer_1.word_index\n",
    "print(word_index_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ef1d1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'how': 1, 'are': 2, 'today.': 3, 'i': 4, 'love': 5, 'money:': 6}\n"
     ]
    }
   ],
   "source": [
    "tokenizer_1 = Tokenizer(num_words = 100, filters = '$%')\n",
    "tokenizer_1.fit_on_texts(sen)\n",
    "word_index_1 = tokenizer_1.word_index\n",
    "print(word_index_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0b12057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 5, 4], [1, 2, 3, 6, 4], [7, 2, 8, 9, 1]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(Sen1)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fedfbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'today': 1, 'is': 2, 'a': 3, 'day': 4, 'sunny': 5, 'autum': 6, 'what': 7, 'it': 8, 'like': 9}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352dee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
